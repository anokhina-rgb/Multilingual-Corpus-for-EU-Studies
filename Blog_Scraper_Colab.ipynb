{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anokhina-rgb/Multilingual-Corpus-for-EU-Studies/blob/main/Blog_Scraper_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2jFngMrhRSL"
      },
      "source": [
        "# üìÑ Blog Scraper for Google Colab\n",
        "This notebook scrapes text content from blog-style websites and saves it as `.txt`, `.docx`, and `.pdf` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h_75WWLhRSO",
        "outputId": "1e40c6cb-a3e6-4ebf-e265-52a7a2596dbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Install necessary packages\n",
        "!pip install -q requests beautifulsoup4 python-docx fpdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMk3R8VwhRSQ"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from docx import Document\n",
        "from fpdf import FPDF\n",
        "from datetime import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LISZwiF4hRSR"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Define scraping and saving functions\n",
        "def scrape_blog(url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        res = requests.get(url, headers=headers, timeout=10)\n",
        "        if res.status_code != 200:\n",
        "            return None, \"Failed to load URL\"\n",
        "\n",
        "        soup = BeautifulSoup(res.text, 'html.parser')\n",
        "        title_tag = soup.find(\"h1\") or soup.title\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"Untitled\"\n",
        "\n",
        "        content_area = soup.find(\"article\") or soup.find(\"div\", class_=\"post\") or soup.body\n",
        "        text_blocks = []\n",
        "\n",
        "        for el in content_area.find_all([\"p\", \"h1\", \"h2\", \"ul\", \"ol\"]):\n",
        "            if el.name.startswith(\"h\"):\n",
        "                text_blocks.append(\"\\n\" + el.get_text(strip=True).upper() + \"\\n\")\n",
        "            elif el.name in [\"ul\", \"ol\"]:\n",
        "                for li in el.find_all(\"li\"):\n",
        "                    text_blocks.append(\"‚Ä¢ \" + li.get_text(strip=True))\n",
        "            else:\n",
        "                para = el.get_text(strip=True)\n",
        "                if len(para) > 30:\n",
        "                    text_blocks.append(para)\n",
        "\n",
        "        full_text = f\"URL: {url}\\nTITLE: {title}\\n\\n\" + \"\\n\\n\".join(text_blocks)\n",
        "        return title, full_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"Error: {str(e)}\"\n",
        "\n",
        "def save_to_txt(title, text, folder):\n",
        "    path = os.path.join(folder, f\"{title[:50]}.txt\")\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "def save_to_docx(title, text, folder):\n",
        "    doc = Document()\n",
        "    doc.add_heading(title, level=1)\n",
        "    doc.add_paragraph(text)\n",
        "    doc.save(os.path.join(folder, f\"{title[:50]}.docx\"))\n",
        "\n",
        "def save_to_pdf(title, text, folder):\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "    for line in text.split('\\n'):\n",
        "        pdf.cell(200, 10, txt=line[:90], ln=True)\n",
        "    pdf.output(os.path.join(folder, f\"{title[:50]}.pdf\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KeKJxBIhRSS"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Scrape from a list of blog URLs\n",
        "urls = [\n",
        "    \"https://poetryfromtheheart123.blogspot.com/\",\n",
        "    \"https://ukrainianstudentvoices.blogspot.com/\",\n",
        "]\n",
        "\n",
        "folder = f\"scraped_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "for url in urls:\n",
        "    title, text = scrape_blog(url)\n",
        "    if title and text:\n",
        "        print(f\"‚úÖ Scraped: {title}\")\n",
        "        save_to_txt(title, text, folder)\n",
        "        save_to_docx(title, text, folder)\n",
        "        save_to_pdf(title, text, folder)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Failed: {url}\\n{text}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}